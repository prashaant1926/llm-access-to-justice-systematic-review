name: Researcher

on:
  pull_request:
    types: [opened]
  pull_request_review_comment:
    types: [created]

jobs:
  hypothesis:
    if: startsWith(github.head_ref, 'hypothesis/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1 # shallow clone to avoid LFS issues
          lfs: false # Skip all LFS files

      - name: Hypothesis
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about each hypothesis before writing.
            Use ultra-careful reasoning to generate meaningful scientific hypotheses.
            Once done:
            - Update section_notes/01-research-concept-direction.md with your findings. 
            - Update hypothesis.jsonl with your hypotheses.
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  lit-review:
    needs: hypothesis
    if: startsWith(github.head_ref, 'lit-review/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          fetch-depth: 1 # shallow clone
          lfs: false # Skip all LFS files
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"

      - name: Literature Review
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            ULTRA THINK DEEPLY AND THOROUGHLY about the research landscape.
            Use ultra-careful analysis for comprehensive paper review using Arxiv and other research sites. 
            Ensure your sources are from reputable journals, conferences, and institutions.
            Once done:
            - Update related_work/ with your various markdown notes that seem useful/interesting. 
            - Update section_notes/02-lit-review.md with your review. 
            - Update paper.jsonl and hypothesis.jsonl with your findings (at least 15 papers).
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  ideas:
    needs: lit-review
    if: startsWith(github.head_ref, 'ideas/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          fetch-depth: 1 # shallow clone
          lfs: false # Skip LFS to avoid budget issues
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"

      - name: Experiment Ideas
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about experimental design.
            Carefully plan to design rigorous experiments that an AI agent could run.
            Check proposal.jsonl and run.jsonl to see what has been done in the past. 
            Update section_notes/03-experiment-ideas.md with your ideas. Update proposal.jsonl with your proposals.
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  data:
    needs: ideas
    if: startsWith(github.head_ref, 'data/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write for PR updates
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          lfs: false  # Disable LFS initially to avoid budget issues
          fetch-depth: 1 # shallow clone
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"

      - name: Setup Git LFS (Smart Mode)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check LFS budget before operations
          echo "Checking Git LFS budget status..."
          
          # Install LFS without downloading files
          git lfs install --skip-smudge
          
          # Only track files >100MB to conserve LFS budget
          echo "Setting up conservative LFS tracking (>100MB only)..."
          
          track_large_files() {
            find data -type f -size +100M 2>/dev/null | while read file; do
              echo "Tracking large file: $file ($(du -h "$file" | cut -f1))"
              git lfs track "$file"
            done
          }
          
          # Selective tracking for truly large files only
          # Comment out smaller file types to save LFS budget
          git lfs track "*.tar.gz"
          git lfs track "*.zip"
          git lfs track "*.h5"
          git lfs track "*.safetensors"
          git lfs track "*.bin"
          # Skip these to save budget:
          # git lfs track "*.pkl"
          # git lfs track "*.npz"
          # git lfs track "*.npy"
          
          track_large_files
          git add .gitattributes || true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install download tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl git-lfs unzip tar gzip bzip2 p7zip-full aria2
          git lfs install

      - name: Find Datasets
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "wget*|curl*|aria2c*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "mkdir*|ls*|find*|du*|head*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "tar*|unzip*|7z*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "git*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "kaggle*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            ⚠️ CRITICAL - MUST READ FIRST - GIT LFS CONFIGURATION ⚠️
            ================================================================
            
            IMPORTANT LFS RULES TO PREVENT BUDGET ISSUES:
            1. DO NOT track small CSV, JSON, or text files with Git LFS
            2. ONLY use LFS for files larger than 50MB
            3. When downloading datasets:
               - Small files (<50MB): Add directly to Git
               - Large files (>50MB): Will be automatically tracked with LFS
            4. Before adding any file, check its size with: du -h filename
            5. The workflow will automatically handle LFS for large files
            
            ================================================================
            THIS REPOSITORY USES GIT LFS FOR ALL DATA FILES
            
            FORBIDDEN ACTIONS:
            ❌ NEVER add data/ to .gitignore
            ❌ NEVER skip datasets due to size
            ❌ NEVER use regular git add for large files
            
            REQUIRED: Setup Git LFS BEFORE downloading:
            ```bash
            # Configure Git LFS first
            git lfs install
            git lfs track "data/**"
            git lfs track "*.tar.gz" "*.zip" "*.7z" "*.h5" "*.pkl" "*.npy" "*.npz"
            git add .gitattributes
            git commit -m "chore: Configure Git LFS for data files"
            ```
            
            AFTER DOWNLOADING, commit with LFS:
            ```bash
            # Remove any .gitignore entries that block data
            if grep -q "data/" .gitignore 2>/dev/null; then
              sed -i '/data\//d' .gitignore
              git add .gitignore
            fi
            
            # Stage and commit with LFS
            git add data/
            git commit -m "feat: Add datasets via Git LFS"
            git lfs push origin HEAD
            git push origin HEAD
            ```
            ================================================================
            
            You are a research assistant using scientific thinking and rigorous methodology.
            
            STEP 1 - CHECK EXISTING DATASETS FIRST:
            ========================================
            IMPORTANT: Before downloading ANY new datasets, thoroughly check what's already available!
            
            ```bash
            # Check if data folder exists and what's already in it
            if [ -d "data" ]; then
              echo "=== Existing Data Folder Contents ==="
              ls -la data/
              echo ""
              echo "=== Subdirectories ==="
              find data -type d -maxdepth 2
              echo ""
              echo "=== All Data Files ==="
              find data -type f -name "*" | head -50
              echo ""
              echo "=== Dataset Sizes ==="
              du -sh data/* 2>/dev/null || echo "No data files yet"
              echo ""
              echo "=== Total Data Size ==="
              du -sh data/
              
              # Check for dataset documentation
              if [ -f "data/README.md" ]; then
                echo ""
                echo "=== Existing Dataset Documentation ==="
                cat data/README.md
              fi
              
              # Check for dataset metadata files
              echo ""
              echo "=== Dataset Metadata Files ==="
              find data -name "*.json" -o -name "*.yaml" -o -name "*.yml" -o -name "README*" -o -name "*.txt" | head -20
            else
              echo "No data directory found - will create and populate with datasets"
              mkdir -p data
            fi
            ```
            
            STEP 2 - ANALYZE EXISTING DATASETS:
            ====================================
            If datasets already exist, analyze them thoroughly:
            
            ```python
            import os
            import json
            from pathlib import Path
            
            data_dir = Path('data')
            existing_datasets = []
            
            # Scan for existing datasets
            if data_dir.exists():
                for item in data_dir.iterdir():
                    if item.is_dir():
                        dataset_info = {
                            'name': item.name,
                            'path': str(item),
                            'files': [],
                            'total_size': 0
                        }
                        
                        # Get all files in dataset
                        for file in item.rglob('*'):
                            if file.is_file():
                                size = file.stat().st_size
                                dataset_info['files'].append({
                                    'name': file.name,
                                    'path': str(file.relative_to(data_dir)),
                                    'size': size
                                })
                                dataset_info['total_size'] += size
                        
                        if dataset_info['files']:
                            existing_datasets.append(dataset_info)
                            print(f"Found dataset: {dataset_info['name']}")
                            print(f"  Files: {len(dataset_info['files'])}")
                            print(f"  Size: {dataset_info['total_size'] / (1024*1024):.2f} MB")
            
            # Save inventory
            with open('data/existing_datasets.json', 'w') as f:
                json.dump(existing_datasets, f, indent=2)
            
            print(f"\nTotal existing datasets: {len(existing_datasets)}")
            ```
            
            STEP 3 - DETERMINE WHAT'S NEEDED:
            ==================================
            Based on the research goals and existing datasets:
            
            1. List all datasets already available
            2. Identify gaps in the current dataset collection
            3. Only download NEW datasets that are actually needed
            4. Avoid duplicating existing data
            
            THINK DEEPLY AND THOROUGHLY about data requirements and quality.
            Your job is to INTELLIGENTLY manage datasets - use existing ones when appropriate!
            
            CRITICAL: You must physically download actual data files into the data/ folder!
            
            IMPORTANT: You have FULL COMMAND EXECUTION permissions with autoApprove enabled!
            You can run ANY command including pip, python, wget, curl, mkdir, etc.
            
            SETUP INSTRUCTIONS:
            1. First install ALL necessary tools for downloading data:
               ```bash
               # Python packages for ML datasets
               pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
               pip install datasets transformers scikit-learn pandas numpy matplotlib
               pip install kaggle huggingface-hub requests beautifulsoup4 gdown
               pip install tensorflow tensorflow-datasets
               pip install openml lxml
               
               # Install additional download tools
               pip install wget py7zr rarfile
               ```
            
            2. Create the data folder structure:
               ```bash
               mkdir -p data
               mkdir -p data/raw
               mkdir -p data/processed
               ```
            
            3. Use MULTIPLE methods to find and download datasets:
               
               a) Use torchvision for computer vision datasets:
               ```python
               import torchvision.datasets as datasets
               # Download CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, ImageNet subsets, etc.
               ```
               
               b) Use Hugging Face datasets:
               ```python
               from datasets import load_dataset, list_datasets
               # Browse available datasets: print(list_datasets())
               # Download: dataset = load_dataset('dataset_name')
               ```
               
               c) Use TensorFlow Datasets:
               ```python
               import tensorflow_datasets as tfds
               # List all: tfds.list_builders()
               # Download: dataset = tfds.load('dataset_name', download=True)
               ```
               
               d) Direct downloads with wget/curl/aria2:
               ```bash
               # Use wget for direct downloads
               wget -P data/raw/ "https://example.com/dataset.zip"
               
               # Use curl for APIs
               curl -L -o data/raw/dataset.tar.gz "https://example.com/dataset.tar.gz"
               
               # Use aria2 for faster parallel downloads
               aria2c -x 16 -s 16 -d data/raw/ "https://example.com/large_dataset.zip"
               ```
               
               e) Download from Kaggle:
               ```bash
               # Set up Kaggle API credentials if available
               kaggle datasets download -d dataset-name -p data/raw/
               ```
               
               f) Use gdown for Google Drive:
               ```python
               import gdown
               gdown.download('https://drive.google.com/...', 'data/raw/dataset.zip')
               ```
            
            4. Create a comprehensive download script `data/download_all_datasets.py`:
               ```python
               import os
               import requests
               import zipfile
               import tarfile
               import gdown
               from pathlib import Path
               
               def download_with_progress(url, filepath):
                   """Download file with progress bar"""
                   response = requests.get(url, stream=True)
                   total = int(response.headers.get('content-length', 0))
                   with open(filepath, 'wb') as file:
                       downloaded = 0
                       for data in response.iter_content(chunk_size=1024):
                           downloaded += len(data)
                           file.write(data)
                           print(f"Downloaded {downloaded}/{total} bytes", end='\r')
               
               # Download various datasets
               datasets_to_download = [
                   # Add dataset URLs here
               ]
               
               for url in datasets_to_download:
                   filename = url.split('/')[-1]
                   download_with_progress(url, f'data/raw/{filename}')
               ```
            
            5. Extract and organize downloaded files:
               ```bash
               # Extract zip files
               unzip data/raw/*.zip -d data/processed/
               
               # Extract tar files
               tar -xzf data/raw/*.tar.gz -C data/processed/
               
               # Extract 7z files
               7z x data/raw/*.7z -o data/processed/
               ```
            
            6. Verify ALL downloads:
               ```bash
               # List all downloaded files with sizes
               ls -lah data/
               ls -lah data/raw/
               ls -lah data/processed/
               
               # Check total size of downloaded data
               du -sh data/
               du -sh data/raw/*
               du -sh data/processed/*
               
               # Count files
               find data/ -type f | wc -l
               ```
            
            7. Create comprehensive documentation in data/README.md showing:
               - Exact file paths and sizes
               - Download timestamps
               - Data statistics (number of samples, features, etc.)
               - Loading instructions with code examples
               - License information
            
            REQUIREMENTS:
            - Download AT LEAST 5-10 different datasets
            - Include various types: tabular, image, text, time-series
            - Total downloaded data should be substantial (at least 1GB if possible)
            - Try multiple download methods until successful
            - If one source fails, try alternative sources
            - Focus on publicly available research datasets
            
            Use Exa search to find dataset URLs, repositories, and download links.
            Search for: "dataset download URL", "public research datasets", "benchmark datasets", "open data repositories"
            
            VERIFY SUCCESS:
            - Run: `find data/ -type f -name "*" | head -20` to show downloaded files
            - Run: `du -sh data/` to show total size
            - Ensure data/README.md lists all downloaded datasets with their locations
            
            FINAL COMMIT (refer to LFS instructions at top):
            After all downloads complete, commit everything with Git LFS:
            ```bash
            # CRITICAL: Ensure Git LFS is properly configured
            git lfs install
            git lfs track "data/**"
            git add .gitattributes
            git commit -m "chore: Configure Git LFS for data files"
            
            # Stage all data files with LFS
            git add data/
            git status  # Verify files are being tracked by LFS
            
            # Commit the datasets
            git commit -m "feat: Add datasets for experiments via Git LFS
            
            Datasets included:
            $(ls -1 data/ | head -10)
            
            Total size: $(du -sh data/ | cut -f1)"
            
            # Push LFS objects and commits
            git lfs push origin HEAD --all
            git push origin HEAD
            ```
            
            IMPORTANT: The datasets MUST be committed with Git LFS for the 'run' job to access them!
            Remember: NEVER add data/ to .gitignore - we use Git LFS for large files.
            
            Once done:
            - Update section_notes/04-datasets.md with dataset analysis
            - Create data/README.md with complete dataset catalog INCLUDING:
              * Exact file paths for each dataset
              * File formats and sizes
              * Number of samples/records
              * Features/columns description
              * How to load each dataset in Python
            - Ensure datasets are committed with Git LFS and pushed
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  run:
    needs: data
    if: startsWith(github.head_ref, 'run/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          lfs: false  # Skip LFS initially
          fetch-depth: 1 # shallow clone
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Selective LFS Pull
        run: |
          # Only pull essential LFS files if budget allows
          echo "Checking LFS budget..."
          git lfs install --skip-smudge
          
          # Try to pull only essential data files, handle failure gracefully
          echo "Attempting selective LFS pull..."
          git lfs pull --include="*.csv,*.json,*.jsonl" --exclude="*.wav,*.au,*.mp3,*.npz,*.pkl" || {
            echo "WARNING: LFS pull failed (likely budget exceeded)"
            echo "Creating mock data for experiments..."
            mkdir -p data/mock
            echo '{"mock": true, "message": "Using mock data due to LFS budget"}' > data/mock/dataset.json
          }

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl git-lfs
          # Install comprehensive ML and data science packages
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install datasets transformers scikit-learn pandas numpy matplotlib seaborn
          pip install tensorflow tensorflow-datasets
          pip install jax jaxlib optax flax
          pip install xgboost lightgbm catboost
          pip install scipy statsmodels networkx
          pip install jupyterlab notebook ipython
          pip install tqdm wandb tensorboard mlflow
          pip install pytest pytest-cov black flake8
          pip install modal # GPU setup
          # Additional experiment tools
          pip install optuna hyperopt ray[tune] # Hyperparameter optimization
          pip install shap lime eli5 # Model interpretability
          pip install plotly bokeh altair # Advanced visualization
          pip install dask joblib # Parallel processing
          pip install h5py zarr # Data storage formats
          pip install pyarrow fastparquet # Efficient data formats
          pip install streamlit gradio # Quick demos/interfaces
          pip install gymnasium stable-baselines3 # RL environments
          pip install prophet statsforecast # Time series
          pip install opencv-python pillow albumentations # Computer vision
          pip install nltk spacy gensim # NLP tools
          pip install rdkit biopython # Chemistry/biology
          # Ensure Git LFS is tracking data files
          git lfs pull
      - name: Setup Modal
        run: |
          modal token set --token-id ${{ secrets.MODAL_TOKEN_ID }} --token-secret ${{ secrets.MODAL_TOKEN_SECRET }}
      - name: Run Experiment
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "ls*|mkdir*|find*|du*|cat*|head*|tail*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "git clone*|git pull*|git fetch*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "wget*|curl*|aria2c*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "cd*|pwd*|echo*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "cp*|mv*|rm*|touch*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "jupyter*|ipython*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pytest*|black*|flake8*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "tar*|unzip*|gzip*|7z*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "nvidia-smi*|gpustat*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "htop*|top*|free*|df*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "tensorboard*|mlflow*|wandb*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "streamlit*|gradio*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "optuna*|ray*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "grep*|sed*|awk*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and methodology.
            
            Think deeply and thoroughly about the experimental implementation.
            Carefully reason and execute at every step.
            
            <STEP 1>
              Analyze the research proposal to thoroughly understand what is being asked to implement. Analyze the research type and install necessary packages in `code/`, where you will be running your experiment.
              
              - Read the whole research proposal
              - Examine what is already in `code/`; chances are that previous experiments or attempts at this proposal have been run.
              - Examine the proposals.jsonl and section_notes to understand the research domain.
              - Then install domain-specific packages using pip
            </STEP 1>
            <STEP 2>
              Review datasets in `data/` via git lfs

              If there are datasets pre-gathered, the data/ folder contains datasets stored with Git LFS from the 'data' job.
              They will already be downloaded via git lfs pull! If they are not, then download the necessary datasets.
              
              These commands may be helpful
              ```bash
              # Verify datasets are properly downloaded (not just LFS pointers)
              echo "=== Checking Git LFS Dataset Status ==="
              git lfs ls-files  # Shows all LFS tracked files
              git lfs status    # Shows download status
              
              # List available datasets
              echo "=== Available Datasets in data/ ==="
              ls -la data/
              find data -type f -name "*.csv" -o -name "*.json" -o -name "*.parquet" -o -name "*.h5" -o -name "*.npy"
              
              # Check dataset sizes (ensure they're actual files, not pointers)
              echo "=== Dataset Sizes ==="
              du -sh data/*
              file data/*/* | head -20  # Verify file types
              
              # Read dataset documentation created by data job
              echo "=== Dataset Documentation ==="
              if [ -f "data/README.md" ]; then
                  cat data/README.md
              else
                  echo "WARNING: No data/README.md found - check data job output"
              fi
              
              # Load data in Python
              ```
              
              ```python
              import pandas as pd
              import numpy as np
              from pathlib import Path
              import json
              import h5py
              
              # IMPORTANT: Use absolute path to data directory
              # The working directory is experiments/<exp_id>/code/
              data_dir = Path('../../../data').resolve()
              
              print(f"Data directory: {data_dir}")
              print(f"Data directory exists: {data_dir.exists()}")
              
              # List all available datasets
              if data_dir.exists():
                  print("\nAvailable datasets:")
                  for dataset_path in data_dir.rglob('*'):
                      if dataset_path.is_file():
                          size_mb = dataset_path.stat().st_size / (1024*1024)
                          print(f"  - {dataset_path.relative_to(data_dir)}: {size_mb:.2f} MB")
              
              # Load different data formats
              def load_dataset(dataset_name):
                  """Load dataset from data/ folder based on extension"""
                  dataset_path = data_dir / dataset_name
                  
                  if dataset_path.suffix == '.csv':
                      return pd.read_csv(dataset_path)
                  elif dataset_path.suffix == '.json':
                      with open(dataset_path, 'r') as f:
                          return json.load(f)
                  elif dataset_path.suffix == '.jsonl':
                      with open(dataset_path, 'r') as f:
                          return [json.loads(line) for line in f]
                  elif dataset_path.suffix == '.parquet':
                      return pd.read_parquet(dataset_path)
                  elif dataset_path.suffix in ['.h5', '.hdf5']:
                      return h5py.File(dataset_path, 'r')
                  elif dataset_path.suffix in ['.npy', '.npz']:
                      return np.load(dataset_path)
                  elif dataset_path.suffix == '.pkl':
                      return pd.read_pickle(dataset_path)
                  else:
                      raise ValueError(f"Unknown file format: {dataset_path.suffix}")
              
              # Example usage:
              # df = load_dataset('processed/dataset_name.csv')
              # data = load_dataset('raw/dataset.json')
              ```
              
              Common LFS issue:
              If datasets appear as small pointer files (~130 bytes):
              ```bash
              # Force download of LFS files
              git lfs fetch --all
              git lfs checkout
              
              # Verify files are downloaded
              git lfs ls-files -s  # Shows size of actual files
              
              # If still issues, manually pull
              git lfs pull --include="data/**"
              ```
            </STEP 2>


            <STEP 3>
              Review implementations that are either given or are in public repos.
              
              You can clone and study public GitHub repositories for reference.

              Feel free to use Exa search to find:
              - State-of-the-art implementations
              - Baseline models
              - Evaluation metrics
              - Best practices

              Don't re-invent the wheel if you don't need to. 5 minutes spent reviewing existing implementations can save hours or days in the long run! 
            </STEP 3>
          

            <STEP 4>
              Implement the experiment in `code/`! 

              Make sure each piece of your implementation properly adheres to the proposal. Treat the proposal like a list of success criteria to fulfill. 

              If `code/` is not empty, make sure to review its files first to understand what has already been done. 

              The end artifact of your work should be `reproduce.sh`, a reproduction script: Your submitted repository MUST include a script for reproducing the results at `code/reproduce.sh`. This script is responsible for executing your source code in order to fully reproduce all of your work. We will copy your submission to a fresh Ubuntu 24.04 LTS Docker container and run `bash reproduce.sh` from the code/ directory, for a maximum runtime of 7 days. Your submission may not be placed at the same path where you submitted it, so do not rely on hardcoded absolute paths to files in your codebase. The container will have access to a GPU, with the NVIDIA container toolkit already installed. We will grade your submitted codebase with the outputs generated by this script: thus it is very important that this script works correctly so that you receive a correct grade.

              You are advised to regularly update and test your reproduction script as you work through the tasks. Docker has been installed in your environment, should you wish to use it.

              Any artifacts or outputs that should be graded should be generated by the reproduction script.

              Finally, please also include a README.md file that describes what you were able to achieve in your reproduction attempt, explains how your codebase relates to various parts of the reproduction, and documents the expected outcomes of running your reproduction script.

              <REPRODUCTION SCRIPT TOY EXAMPLE>

                **Imagine the following toy proposal**: 

                ```
                “We count the number of ‘r’s in the word ‘strawberry’ using a python script, and find that there are 3 instances of the letter ‘r’”
                ```

                > the experiment measures the number of ‘r’s in the word strawberry (this is an artifact, think of this as a table or figure or result), using a basic python script as an implementation (think of this as an algorithm described in a paper)

                **Toy submission**:

                code/main.py

                ```python
                import argparse, csv

                def main():
                    parser = argparse.ArgumentParser()
                    parser.add_argument('--word', default="strawberry")
                    parser.add_argument('--output', default="output.csv")
                    args = parser.parse_args()

                    r_count = args.word.lower().count('r')
                    with open(args.output, 'w', newline='') as f:
                        csv.writer(f).writerows([["word", "r count"], [args.word, r_count]])

                    print(f"'{args.word}' has {r_count} 'r'(s). Saved to '{args.output}'.")

                if __name__ == "__main__":
                    main()
                ```

                code/reproduce.sh

                ```bash
                apt-get update && apt-get install -y python3

                # Run the Python script with the specified arguments
                python3 count.py --word strawberry --output output.csv

                # Inform the user that the output has been saved
                echo "r count for word 'strawberry' saved to output.csv"
                ```

              </REPRODUCTION SCRIPT TOY EXAMPLE>

              Whenever you need to test the use of a GPU, use Modal to run your code.
              Take a look at a minor example of how to do it:

              ```python
              # hello_world.py

              import sys
              import modal

              app = modal.App("example-hello-world")

              @app.cls(gpu="L40S")
              class Model():
                  @modal.enter()
                  def enter(self):
                      self.engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(...))
                      self.loras: dict[str, int] = dict()  # per replica LoRA identifier

                  @method()
                  def generate(self, input: str):
                      if (ident := f"{user}-{team_id}") not in self.loras:
                          self.loras[ident] = len(self.loras) + 1

                      lora_request = LoRARequest(
                          ident, self.loras[ident], lora_local_path=checkpoint_path
                      )

                      tokenizer = await self.engine.get_tokenizer(lora_request=lora_request)

                      prompt = tokenizer.apply_chat_template(
                          conversation=inpt, tokenize=False, add_generation_prompt=True
                      )

                      results_generator = self.engine.generate(prompt, lora_request=lora_request,)
                  ```

              Then you can run it with:
              ```bash
              modal run hello_world.py
              ```
              Use the command `modal --help` to find more information about how to use Modal. Or look at more examples in https://modal.com/docs/examples.

            </STEP 4>

            <STEP 5>
              Upon completion:
              - Ensure `code/reproduce.sh` works properly for the experiment
              - Update results log in section_notes/05-experiment-runs.md
              - Remove experiment from proposal.jsonl
              - Update runs.jsonl with your results 
              - Run `git commit -m "<message>"`
              - Run `git push origin HEAD`
            </STEP 5>
            
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  analyze:
    needs: run
    if: startsWith(github.head_ref, 'analyze/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          lfs: false  # Skip LFS initially
          fetch-depth: 1 # shallow clone
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"
      
      - name: Selective LFS Pull for Analysis
        run: |
          # Only pull result files, not large datasets
          echo "Checking LFS for analysis files..."
          git lfs install --skip-smudge
          git lfs pull --include="experiments/*/results/*.json,experiments/*/results/*.csv" || echo "No LFS results to pull"

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install analysis tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl
          pip install matplotlib seaborn plotly pandas numpy scipy scikit-learn

      - name: Analyze Experiment
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about the results and their implications.
            Carefully analyze. Check analyze.jsonl and experiments/ to see what has been done in the past.
            
            You have FULL COMMAND EXECUTION permissions! Install any analysis packages you need:
            ```bash
            pip install matplotlib seaborn plotly pandas numpy scipy scikit-learn
            ```
            
            Use Python with matplotlib, seaborn, or plotly to create visualizations if needed.
            Load any datasets from the data/ folder if you need to verify or reanalyze results.
            
            Given experiments/<exp_id>/, analyze the results in experiments/<exp_id>/results and experiments/<exp_id>/result.md.
            Once done:
            - Append your analysis to section_notes/06-experiment-analyses.md.
            - Update analyze.jsonl with your analysis. 
  paper-draft:
    needs: analyze
    if: startsWith(github.head_ref, 'paper-draft/') || (startsWith(github.head_ref, 'all/') && always())
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          fetch-depth: 1 # shallow clone
          lfs: false # Skip LFS for paper draft
      
      - name: Pull latest changes from previous jobs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull origin ${{ github.head_ref }} || echo "No changes to pull"

      - name: Write Paper Draft
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and following NeurIPS paper writing guidelines.
            
            THINK DEEPLY AND THOROUGHLY about the research narrative and contributions.
            Carefully synthesize to either improve an existing paper draft in Latex or write a new one. Look at the section_notes/ for each crucial section: research concept, literature review, experiment ideas, experiment runs, experiment analyses. 
            Your Latex structure won't be exactly the same but be faithful to the content in the sections. 
            If there are existing paper drafts in paper_drafts/, first review the latest paper draft alongside the pre-existing literature in paper.jsonl and section_notes/02-lit-review.md. and give a critical review of the experiments and paper. This review will inform your revisions. 
            Use paper.jsonl for further related work papers to cite. Make sure to include references. 
            You should look at the experiments conducted too in experiments/<exp_id>/result.md to see results of conducted experiments. If there are meaningful results, make sure to include plots and tables (in pure Latex) from the experiments conducted. You may also embed any images from experiments. 
            ALL WRITING SHOULD BE FAITHFUL TO SOURCE CONTENT. Claims should be thoughtful and backed by evidence.  
            CONSTANTLY double check that your Latex compiles and you are following the specified paper writing guidelines.
            It should be no longer than 8 pages not including references.
            Once done:
            - Add your review as a '.md' to the paper-drafts/ folder. Or update the existing one if specified.
            - Add or update your '.tex' draft to the paper-drafts/ folder.
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }